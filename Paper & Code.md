# Content
1. [[Remote Sensing](#Remote_Sensing)]
2. [[Training Free Segmentation](#Training_Free)]
3. [[segmentation and detection](#Segmentation_and_Detection)]
4. [[Open vocabulary](#open_vocabulary)]


<a name="Remote_Sensing"></a> 
## Remote Sensing
1. [2025 arXiv] **DynamicEarth: How Far are We from Open-Vocabulary Change Detection?** [[paper]](https://arXiv.org/abs/2501.12931) [[code]](https://github.com/likyoo/DynamicEarth)
2. [2025 ICCV] **Dynamic Dictionary Learning for Remote Sensing Image Segmentation.** [[paper]](https://arXiv.org/pdf/2503.06683) [[code]](https://github.com/XavierJiezou/D2LS)
3. [2025 ICCV] **GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks.** [[paper]](https://arxiv.org/pdf/2411.19325) [[code]](https://github.com/The-AI-Alliance/GEO-Bench-VLM)
4. [2025 ICCV] **SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation.** [[paper]](https://arXiv.org/abs/2507.12857) [[code]](https://github.com/HuangShiqi128/SCORE)
5. [2025 ICCV] **When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning.** [[paper]](https://arXiv.org/pdf/2503.07588) [[code]](https://github.com/VisionXLab/LRS-VQA)
6. [2025 AAAI] **ZoRI: Towards discriminative zero-shot remote sensing instance segmentation.** [[paper]](https://arXiv.org/abs/2412.12798) [[code]](https://github.com/HuangShiqi128/ZoRI)
7. [2024 NIPS] **Segment Any Change.** [[paper]](https://proceedings.NIPS.cc/paper_files/paper/2024/file/9415416201aa201902d1743c7e65787b-Paper-Conference.pdf) [[code]](https://github.com/Z-Zheng/pytorch-change-models)
8. [2025 CVPR] **SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images.** [[paper]](https://arXiv.org/abs/2410.01768) [[code]](https://github.com/likyoo/SegEarth-OV)
9. [2025 CVPR] **XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?** [[paper]](https://arXiv.org/abs/2503.23771) [[code]](https://github.com/EvolvingLMMs-Lab/XLRS-Bench)
10. [2025 CVPR] **Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation.** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image_CVPR_2025_paper.pdf) [[code]](https://github.com/MiSsU-HH/Exact)
11. [2025 Arxiv] **SegEarth-OV-2: Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images** [[paper]](https://arxiv.org/abs/2508.18067)  [[code]](https://github.com/earth-insights/SegEarth-OV-2)
12. [2025 AAAI] **Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation** [[paper]](https://arxiv.org/abs/2412.19492) [[code]](https://github.com/yecy749/GSNet)
13. [2025 Arxiv] **InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition** [[paper]](https://arxiv.org/pdf/2505.15818) [[code]](https://github.com/VoyagerXvoyagerx/InstructSAM)
14. [2025 Arxiv] **DescribeEarth: Describe Anything for Remote Sensing Images** [[paper]](https://arxiv.org/pdf/2509.25654v1) [[code]](https://github.com/earth-insights/DescribeEarth)
15. [2025 NIPS] **GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset** [[paper]](https://arxiv.org/abs/2507.14697) [[code]](https://github.com/Z-ZW-WXQ/GTPBD)
16. [2025 TGRS] **Semantic Prototyping With CLIP for Few-Shot Object Detection in Remote Sensing Images** [[paper]](https://ieeexplore.ieee.org/document/10930588)
17. [2025 Arxiv] **Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing** [[paper]](https://arxiv.org/pdf/2509.12040)[[code]](https://github.com/LiBingyu01/RSKT-Seg?tab=readme-ov-file)
18. [2024 Arxiv] **Segearth-ov: Towards training-free open-vocabulary segmentation for remote sensing images**[[paper]](https://arxiv.org/pdf/2410.01768)
19. [2025 Arxiv] **Open-vocabulary remote sensing image semantic segmentation.**[[paper]](https://arxiv.org/pdf/2409.07683)
20. [2025 Arxiv] **SegEarth-OV-2: Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images**[[paper]](https://arxiv.org/abs/2508.18067)[[code]](https://github.com/earth-insights/SegEarth-OV-2)
21. [2025 Arxiv] **ATRNet-STAR: A Large Dataset and Benchmark Towards Remote Sensing Object Recognition in the Wild** [[paper]](https://arxiv.org/abs/2501.13354) [[code]](https://github.com/waterdisappear/ATRNet-STAR)
22. [2025 Arxiv] **AlignCLIP: Self-Guided Alignment for Remote Sensing Open-Vocabulary Semantic Segmentation** [[paper]](https://openreview.net/forum?id=hpD3tn7Xbp) [[code]](https://openreview.net/attachment?id=hpD3tn7Xbp&name=supplementary_material)
23. [2025 Arxiv] **Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models**[[paper]](https://arxiv.org/pdf/2510.07135) [[code]](https://github.com/elkhouryk/fewshot_RSVLMs)
24. [2025 Arxiv] **Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models**[[paper]](https://arxiv.org/abs/2510.13993)
25. [2025 RSE] **Strategic sampling for training a semantic segmentation model in operational mapping: Case studies on cropland parcel extraction**[[paper]](https://doi.org/10.1016/j.rse.2025.115034) [[code]](https://github.com/Remote-Sensing-of-Land-Resource-Lab/Training-Sample-Selection)
26. [2025 CVMJ] **Remote sensing tuning: A survey**[[paper]](https://ieeexplore.ieee.org/document/11119145)[[paper]](https://github.com/DongshuoYin/Remote-Sensing-Tuning-A-Survey)


<a name="Training_Free"></a>
## Training Free Segmentation
### VLM Only
1. [2023 arXiv] **CLIPSurgery: CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks.** [[paper]](https://arXiv.org/pdf/2304.05653) [[code]](https://github.com/xmed-lab/CLIP_Surgery)
2. [2024 arXiv] **SC-CLIP: Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation.** [[paper]](https://arXiv.org/pdf/2411.15869) [[code]](https://github.com/SuleBai/SC-CLIP)
3. [2025 arXiv] **A Survey on Training-free Open-Vocabulary Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2505.22209)
4. [2022 ECCV] **Maskclip: Extract Free Dense Labels from CLIP.** [[paper]](https://arXiv.org/pdf/2112.01071) [[code]](https://github.com/chongzhou96/MaskCLIP)
5. [2024 ECCV] **Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2407.08268) [[code]](https://github.com/leaves162/CLIPtrase)
6. [2024 ECCV] **ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference.** [[paper]](https://arXiv.org/pdf/2407.12442) [[code]](https://github.com/mc-lan/ClearCLIP)
7. [2025 AAAI] **Unveiling the Knowledge of CLIP for Training-Free Open-Vocabulary Semantic Segmentation.** [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/32602) [[code]](https://ojs.aaai.org/index.php/AAAI/article/view/32602)
8. [2024 WACV] **NACLIP: Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2404.08181) [[code]](https://github.com/sinahmr/NACLIP)
9. [2024 ICLR] **Vision Transformers Need Registers.** [[paper]](https://arXiv.org/pdf/2309.16588) [[code]](https://github.com/kyegomez/Vit-RGTS)
10. [2024 ICLR] **Vision Transformers Don't Need Trained Registers.** [[paper]](https://arXiv.org/pdf/2506.08010) [[code]](https://github.com/nickjiang2378/test-time-registers/tree/main)
11. [2025 arXiv] **Post-Training Quantization of Vision Encoders Needs Prefixing Registers** [[paper]](https://arxiv.org/pdf/2510.04547v1)
12. [2025 arXiv] **To sink or not to sink: visual information pathways in large vision-language models** [[paper]](https://arxiv.org/pdf/2510.08510v1)
13. [2025 CVPR] **ResCLIP: Residual Attention for Training-free Dense Vision-language Inference.** [[paper]](https://arXiv.org/pdf/2411.15851) [[code]](https://github.com/yvhangyang/ResCLIP)
14. [2024 CVPR] **GEM: Grounding Everything: Emerging Localization Properties in Vision-Language Transformers.** [[paper]](https://arXiv.org/pdf/2312.00878) [[code]](https://github.com/WalBouss/GEM)
15. [2025 CVPR] **ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements.** [[paper]](https://arXiv.org/pdf/2411.12044) [[code]](https://github.com/m-arda-aydn/ITACLIP)
16. [] **Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification** [[paper]](https://arxiv.org/pdf/2207.09519)



### VLM & VFM & Diffusion & SAM
1. [2025 ICCV] **Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation.** [[paper]](https://arXiv.org/pdf/2411.19331) [[code]](https://github.com/lorebianchi98/Talk2DINO)
2. [2025 ICCV] **ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation.** [[paper]](https://arXiv.org/pdf/2506.21233) [[code]](https://github.com/xiweix/ReME)
3. [2025 ICCV] **FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2504.10487) [[code]](https://github.com/yasserben/FLOS
4. [2025 ICCV] **Trident: Harnessing Vision Foundation Models for High-Performance, Training-Free Open Vocabulary Segmentation.** [[paper]](https://arXiv.org/pdf/2411.09219) [[code]](https://github.com/YuHengsss/Trident)
5. [2024 AAAI] **TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training.** [[paper]](https://arXiv.org/pdf/2312.12828) [[code]](https://github.com/linyq2117/TagCLIP)
6. [2024 CVPR] **FreeDA: Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation.** [[paper]](https://arXiv.org/pdf/2404.06542) [[code]](https://github.com/aimagelab/freeda)
7. [2025 CVPR] **GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery.** [[paper]](https://arXiv.org/pdf/2403.09974) [[code]](https://github.com/enguangW/GET)
8. [2025 CVPR] **CCD: Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification.** [[paper]](https://arXiv.org/pdf/2503.16873) [[code]](https://github.com/k0u-id/CCD)
9. [2025 CVPR] **SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models.** [[paper]](https://arXiv.org/pdf/2502.16911?) [[code]](https://github.com/kjmillerCURIS/SPARC)
10. [2025 CVPR] **LOPSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2503.19777) [[code]](https://github.com/vladan-stojnic/LPOSS/tree/main)
11. [2025 arXiv] **One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework** [[paper]](https://arxiv.org/pdf/2510.02898v1) [[code]](https://paciosoft.com/Patch-ioner/)


<a name="Segmentation_and_Detection"></a>
## Segmentation and Detection
1. [2015 CVPR] **FCN: Fully Convolutional Networks for Semantic Segmentation.** [[paper]](https://arXiv.org/abs/1411.4038) [[code]](https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn)
2. [2016 MICCAI] **UNet: Convolutional Networks for Biomedical Image Segmentation.** [[paper]](https://arXiv.org/pdf/1505.04597)
3. [2017 arXiv] **DeepLabV3: Rethinking atrous convolution for semantic image segmentation.** [[paper]](https://arXiv.org/pdf/1706.05587)
4. [2018 CVPR] **DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.** [[paper]](https://arXiv.org/pdf/1802.02611)
5. [2019 CVPR] **Semantic FPN: Panoptic Feature Pyramid Networks.** [[paper]](https://arXiv.org/pdf/1901.02446)
6. [2021 CVPR] **SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers.** [[paper]](https://arXiv.org/pdf/2012.15840) [[code]](https://github.com/fudan-zvg/SETR)
7. [2021 ICCV] **Segmenter: Transformer for Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2105.05633) [[code]](https://github.com/rstrudel/segmenter)
8. [2021 NIPS] **SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.** [[paper]](https://arXiv.org/pdf/2105.15203) [[code]](https://github.com/NVlabs/SegFormer)
9. [2021 CVPR] **MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2107.06278) [[code]](https://github.com/facebookresearch/MaskFormer)
10. [2022 CVPR] **Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation.** [[paper]](https://arXiv.org/pdf/2112.01527) [[code]](https://github.com/facebookresearch/Mask2Former)
11. [2024 CVPR] **Rein: Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2312.04265) [[code]](https://github.com/w1oves/Rein)
12. [2015 NIPS] **Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.** [[paper]](https://arXiv.org/pdf/1506.01497)
13. [2020 ECCV] **DETR: End-to-End Object Detection with Transformers.** [[paper]](https://arXiv.org/pdf/2005.12872) [[code]](https://github.com/facebookresearch/detr)
14. [2021 ICLR] **Deformable DETR: Deformable Transformers for End-to-End Object Detection.** [[paper]](https://arXiv.org/pdf/2010.04159) [[code]](https://github.com/fundamentalvision/Deformable-DETR)
15. [2023 ICLR] **DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.** [[paper]](https://arXiv.org/pdf/2203.03605) [[code]](https://github.com/IDEA-Research/DINO)


<a name="open_vocabulary"></a>
## Open vocabulary
### segmentation
1. [2022 CVPR] **GroupViT: Semantic Segmentation Emerges from Text Supervision (Open-Vocabulary Zero-Shot).** [[paper]](https://arXiv.org/pdf/2202.11094) [[code]](https://github.com/NVlabs/GroupViT)
2. [2022 ECCV] **OpenSeg: Scaling Open-Vocabulary Image Segmentation with Image-Level Labels.** [[paper]](https://arXiv.org/pdf/2112.12143)
3. [2023 CVPR] **FreeSeg: Unified, Universal, and Open-Vocabulary Image Segmentation.** [[paper]](https://arXiv.org/pdf/2303.17225) [[code]](https://github.com/bytedance/FreeSeg)
4. [2023 ICML] **SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation (Zero-Shot).** [[paper]](https://arXiv.org/pdf/2211.14813) [[code]](https://github.com/ArrowLuo/SegCLIP)
5. [2023 CVPR] **ODISE: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models.** [[paper]](https://arXiv.org/pdf/2303.04803) [[code]](https://github.com/NVlabs/ODISE)
6. [2023 ICML] **MaskCLIP: Open-Vocabulary Universal Image Segmentation with MaskCLIP.** [[paper]](https://arXiv.org/pdf/2208.08984) [[code]](https://github.com/mlpc-ucsd/MaskCLIP)
7. [2023 CVPR] **SAN: Side Adapter Network for Open-Vocabulary Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2302.12242) [[code]](https://github.com/MendelXu/SAN)
8. [2024 ECCV] **CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation.** [[paper]](https://arXiv.org/pdf/2312.12359) [[code]](https://github.com/wysoczanska/clip_dinoiser)
9. [2024 CVPR] **SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation.** [[paper]](https://arXiv.org/pdf/2311.15537) [[code]](https://github.com/xb534/SED)
10. [2024 TPAMI] **Review: Towards Open Vocabulary Learning: A Survey.** [[paper]](https://arXiv.org/pdf/2306.15880) [[code]](https://github.com/jianzongwu/Awesome-Open-Vocabulary)
11. [2025 ICCV] **Unbiased Region-Language Alignment for Open-Vocabulary Dense Prediction.** [[paper]](https://arXiv.org/abs/2412.06244) [[code]](https://github.com/HVision-NKU/DenseVLM)
12. [2025 CVPR] **DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception.** [[paper]](https://arXiv.org/pdf/2505.04410) [[code]](https://github.com/xiaomoguhz/DeCLIP)
13. [2025 arXiv] **REFAM: Attention magnets for zero-shot referral segmentaion** [[paper]](https://arxiv.org/pdf/2509.22650v1)


### object detection
1. [2021 CVPR] **Open-Vocabulary Object Detection Using Captions.** [[paper]](https://arXiv.org/pdf/2011.10678) [[code]](https://github.com/alirezazareian/ovr-cnn)
2. [2022 ICLR] **ViLD: Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation.** [[paper]](https://arXiv.org/pdf/2104.13921) [[code]](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)
3. [2022 CVPR] **GLIP: Grounded Language-Image Pre-training.** [[paper]](https://arXiv.org/pdf/2112.03857) [[code]](https://github.com/microsoft/GLIP)
4. [2022 NIPS] **GLIPv2: Unifying Localization and Vision-Language Understanding.** [[paper]](https://arXiv.org/pdf/2206.05836) [[code]](https://github.com/microsoft/GLIP)
5. [2024 ICCV] **Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection** [[paper]](https://arxiv.org/pdf/2303.05499) [[code]](https://github.com/IDEA-Research/GroundingDINO)














